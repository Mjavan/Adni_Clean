{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding novelty to the existing method "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Short answer: **just fine-tuning on two groups and then doing label-based saliency (backprop of a class logit) is *not* novel** on its own. That’s a very standard pipeline (train 2-class model → Grad-CAM/IG on the positive class). Reviewers will ask: *what’s new beyond applying known XAI to a new dataset?*\n",
    "\n",
    "You *can* make it into a publishable contribution by adding one (or more) of these **novelty levers**:\n",
    "\n",
    "## How to turn it into something new\n",
    "\n",
    "1. **Group-level explainability (not just per-sample)**\n",
    "\n",
    "   * Backprop a **group statistic** (e.g., (T^2=|\\mu_A-\\mu_B|^2)) or a **contrastive margin** rather than a single logit.\n",
    "   * Show **quantitative curves**: corruption level ↑ ⇒ statistic ↑, IoU/pointing with known mask ↑, ΔT(top-q) − ΔT(random) ↑.\n",
    "   * This moves you beyond “pretty maps” into **population differences**.\n",
    "\n",
    "2. **Artifact-free positive controls (methodological contribution)**\n",
    "\n",
    "   * Replace zero-out with your **smooth blur/attenuation** control and **prove** CAM/IG recover the injected signal monotonically.\n",
    "   * Package this as a **recommended sanity protocol** for medical XAI. (Most papers still use zeroing—this is a useful fix.)\n",
    "\n",
    "3. **ROI-anchored validation (clinical meaning)**\n",
    "\n",
    "   * Quantify **% attribution inside hippocampus/temporal ROIs**, **pointing rate to ROI**, and **ΔT when masking ROI vs random** on ADNI.\n",
    "   * If AD maps consistently load on known anatomy, that’s meaningful (even if the model is simple).\n",
    "\n",
    "4. **Method agreement & faithfulness**\n",
    "\n",
    "   * Compare **Grad-CAM/Grad-CAM++/IG/occlusion** on both control and ADNI.\n",
    "   * Report **agreement metrics** (Spearman on pixel importances, top-q IoU) and **which method best predicts ΔT** (faithfulness).\n",
    "\n",
    "5. **Probe vs end-to-end**\n",
    "\n",
    "   * Show that a **frozen encoder + linear probe** yields **more stable/faithful attributions** than end-to-end fine-tuning (less overfit, better ΔT/ROI metrics).\n",
    "   * That’s a practical takeaway for the community.\n",
    "\n",
    "6. **Uncertainty-aware explanations (if you have it)**\n",
    "\n",
    "   * If your background includes Bayesian DL: relate **predictive uncertainty** to **attribution stability** (e.g., maps are unstable when uncertainty high). Even a small analysis here is interesting.\n",
    "\n",
    "## Minimal publishable package (what to actually do)\n",
    "\n",
    "* Train: frozen encoder + **linear probe** for A vs B.\n",
    "* Controls: run your **blur/attenuation** positive control with 3 strengths; show **statistic↑, IoU↑, ΔT↑**.\n",
    "* Methods: Grad-CAM++ and **IG (+SmoothGrad)**.\n",
    "* ADNI: compute **energy-in-ROI**, **pointing@ROI**, **ΔT(top-q ROI) vs random** on confident correct cases.\n",
    "* Report **agreement** between methods and which best predicts ΔT.\n",
    "* 6–8 clear figures + 2 tables → workshop/journal short paper.\n",
    "\n",
    "## Bottom line\n",
    "\n",
    "* **“Fine-tune + saliency” alone → not novel.**\n",
    "* **Add**: positive-control protocol + group-level/ROI metrics + method comparison tied to ΔT → **becomes a real contribution**.\n",
    "\n",
    "If you tell me which of these levers you’re willing to include (e.g., positive control + ROI metrics), I’ll outline the exact figures and results section headings so you can assemble a tidy 4–6 week paper plan.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Points about fine-tuning**:\n",
    "\n",
    "- Short answer: **Yes** — what you have is essentially a **ResNet-50 encoder without the SimCLR projection head** + an **avg-pool** that gives you a 2048-D vector. Adding a `Linear(2048, num_classes)` on top is the correct fine-tuning/probing setup.\n",
    "\n",
    "Here’s a clean, reliable way to wire it up (and a couple of small improvements):\n",
    "\n",
    "### Minimal fine-tune head (recommended)\n",
    "\n",
    "```python\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "class FineTuneHead(nn.Module):\n",
    "    def __init__(self, encoder, num_classes=2, imagenet_norm=True, in_channels=1, p_drop=0.2):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder            # your Sequential(...) ending with AdaptiveAvgPool2d(1)\n",
    "        self.imagenet_norm = imagenet_norm\n",
    "        if imagenet_norm:\n",
    "            self.register_buffer(\"mean\", torch.tensor([0.485,0.456,0.406]).view(1,3,1,1))\n",
    "            self.register_buffer(\"std\",  torch.tensor([0.229,0.224,0.225]).view(1,3,1,1))\n",
    "        self.in_channels = in_channels\n",
    "        # Optional small head to stabilize training\n",
    "        self.dropout = nn.Dropout(p_drop)\n",
    "        self.fc = nn.Linear(2048, num_classes)     # ResNet-50 -> 2048-D after avgpool\n",
    "\n",
    "    def forward(self, x01):                         # x01 in [0,1], shape [B,1,H,W] or [B,3,H,W]\n",
    "        if self.in_channels == 1 and x01.size(1) == 1:\n",
    "            x = x01.repeat(1, 3, 1, 1)             # grayscale -> 3ch\n",
    "        else:\n",
    "            x = x01\n",
    "        if self.imagenet_norm:\n",
    "            x = (x - self.mean) / self.std\n",
    "\n",
    "        feats = self.encoder(x)                    # [B, 2048, 1, 1]\n",
    "        if feats.dim() == 4:\n",
    "            feats = feats.flatten(1)               # [B, 2048]\n",
    "        feats = self.dropout(feats)\n",
    "        logits = self.fc(feats)                    # [B, num_classes]\n",
    "        return logits, feats\n",
    "```\n",
    "\n",
    "### Freezing / unfreezing (choose one)\n",
    "\n",
    "* **Linear probe first (safe baseline):**\n",
    "\n",
    "  ```python\n",
    "  for p in model.encoder.parameters(): p.requires_grad = False   # freeze encoder\n",
    "  # train only model.fc (and dropout is stateless)\n",
    "  ```\n",
    "* **If needed, unfreeze last block later (ResNet-50’s layer4):**\n",
    "\n",
    "  ```python\n",
    "  # encoder is your big Sequential; index 7 is layer4 in your printout\n",
    "  for p in model.encoder[0][7].parameters(): p.requires_grad = True\n",
    "  ```\n",
    "\n",
    "  Use a **smaller LR** for encoder params than for `fc`.\n",
    "\n",
    "### Optimizer example\n",
    "\n",
    "```python\n",
    "# probe only\n",
    "opt = torch.optim.AdamW(model.fc.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "# partial unfreeze (layer4 + fc)\n",
    "params = [\n",
    "    {\"params\": model.fc.parameters(),            \"lr\": 1e-3},\n",
    "    {\"params\": model.encoder[0][7].parameters(),\"lr\": 1e-4},  # last block\n",
    "]\n",
    "opt = torch.optim.AdamW(params, weight_decay=1e-4)\n",
    "```\n",
    "\n",
    "### BatchNorm tip\n",
    "\n",
    "If most of the encoder is frozen, set **encoder BN to eval** to keep running stats fixed:\n",
    "\n",
    "```python\n",
    "model.train()\n",
    "model.encoder.eval()         # freeze BN stats in encoder\n",
    "model.fc.train()\n",
    "```\n",
    "\n",
    "(If you unfreeze layer4, `encoder[0][7]` will still update BN in that block.)\n",
    "\n",
    "### For Grad-CAM target layer (naming)\n",
    "\n",
    "Given your structure:\n",
    "\n",
    "* Encoder `Sequential` → `... (7): Sequential(...)` is **layer4**; the **last bottleneck** inside `(7)` is typically the best Grad-CAM target.\n",
    "* Reasonable target names (depending on how you registered hooks):\n",
    "\n",
    "  * Entire block: `encoder[0][7]`\n",
    "  * Or inside last bottleneck: `encoder[0][7][2].conv3` (before avgpool)\n",
    "    Pick the **last high-res conv** in layer4; avoid the avgpool (no spatial map).\n",
    "\n",
    "### TL;DR\n",
    "\n",
    "* Your architecture (ResNet-50 encoder ending with `AdaptiveAvgPool2d(1)` + `Linear(2048, C)`) is **correct**.\n",
    "* Handle grayscale→3ch + normalization inside the model.\n",
    "* Start with **linear probe (freeze encoder)**; only unfreeze **layer4** if needed.\n",
    "* For Grad-CAM, hook a **conv in layer4**; for IG/other methods, use the **logit** (or margin) as the scalar.\n",
    "\n",
    "If you want, paste how you’re currently instantiating the model and I’ll give you the exact lines to set `target_layer` for Grad-CAM and the two optimizer setups (probe vs partial unfreeze).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Fine Tuning checklist**:\n",
    "\n",
    "Sounds good — finish fine-tuning first. Here’s a tight checklist so you don’t miss anything:\n",
    "\n",
    "## Fine-tuning checklist (ResNet50 encoder + linear head)\n",
    "\n",
    "1. **Data & preprocessing**\n",
    "\n",
    "* Inputs → scale to `[0,1]` (float32).\n",
    "* Grayscale → repeat to 3ch.\n",
    "* Normalization → ImageNet mean/std (since encoder is ImageNet/SimCLR-based).\n",
    "* Augment lightly: horizontal flip, small rotation (±5–10°), slight Gaussian blur. Avoid color jitter.\n",
    "\n",
    "2. **Model wiring**\n",
    "\n",
    "* Encoder = SimCLR backbone **without** projector (your printout is correct).\n",
    "* Head = `Linear(2048, 2)` (+ optional `Dropout(p=0.2)` before it).\n",
    "* Start with **encoder frozen** (linear probe). If needed, unfreeze **layer4** later with 10× smaller LR.\n",
    "\n",
    "3. **BatchNorm behavior**\n",
    "\n",
    "* If encoder mostly frozen: `encoder.eval()` (freeze BN running stats) while training the head.\n",
    "* If you unfreeze layer4: keep `model.train()`; BN in layer4 will update.\n",
    "\n",
    "4. **Optimizer / LRs**\n",
    "\n",
    "* Probe only: `AdamW(head, lr=1e-3, weight_decay=1e-4)`.\n",
    "* Partial unfreeze:\n",
    "\n",
    "  * head: `1e-3`\n",
    "  * encoder.layer4: `1e-4`\n",
    "* LR schedule (optional): cosine or StepLR(γ=0.1 at epoch 10).\n",
    "\n",
    "5. **Loss & imbalance**\n",
    "\n",
    "* `CrossEntropyLoss(weight=class_weights)` if classes are imbalanced.\n",
    "* Compute `class_weights = 1 / freq` (normalized).\n",
    "\n",
    "6. **Training loop (essentials)**\n",
    "\n",
    "* Mixed precision if available (`torch.cuda.amp.autocast`, `GradScaler`).\n",
    "* Early stopping on **val loss** with patience 5.\n",
    "* Save **best checkpoint** by val accuracy or loss.\n",
    "\n",
    "7. **Validation you actually need**\n",
    "\n",
    "* Accuracy, precision/recall/F1 per class.\n",
    "* ROC-AUC (binary).\n",
    "* Confusion matrix.\n",
    "* **Embedding check:** collect `feats` from the model, run PCA/UMAP; you should see better separation than frozen encoder.\n",
    "\n",
    "8. **Quick targets (realistic)**\n",
    "\n",
    "* AD vs HC: probe 65–75%; partial unfreeze 70–80% (depends on preprocessing/split).\n",
    "* Don’t chase 90% — focus on stable behavior and sensible maps later.\n",
    "\n",
    "### Minimal code skeleton (probe only)\n",
    "\n",
    "```python\n",
    "model = FineTuneHead(encoder, num_classes=2, imagenet_norm=True, in_channels=1).to(device)\n",
    "for p in model.encoder.parameters(): p.requires_grad = False\n",
    "opt = torch.optim.AdamW(model.fc.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
    "\n",
    "best_val = float('inf'); best_state=None\n",
    "for epoch in range(E):\n",
    "    model.train(); model.encoder.eval()\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device).float(), yb.to(device)\n",
    "        logits, feats = model(xb)\n",
    "        loss = loss_fn(logits, yb)\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "    # validate\n",
    "    model.eval(); val_loss=0; correct=0; n=0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(device).float(), yb.to(device)\n",
    "            logits, feats = model(xb)\n",
    "            val_loss += loss_fn(logits, yb).item()*xb.size(0)\n",
    "            pred = logits.argmax(1); correct += (pred==yb).sum().item(); n+=xb.size(0)\n",
    "    val_loss/=n; val_acc=correct/n\n",
    "    if val_loss < best_val: best_val=val_loss; best_state= {k:v.cpu() for k,v in model.state_dict().items()}\n",
    "# load best\n",
    "model.load_state_dict({k:v.to(device) for k,v in best_state.items()})\n",
    "```\n",
    "\n",
    "When this is done and accuracy looks reasonable, we’ll hook Grad-CAM/IG onto the **head logit** and generate maps. If you hit any glitch (loss not decreasing, val worse than train, BN instability), ping me with the symptom and I’ll help you fix it fast.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explaination about accuracy on frozen backbon + linear prob "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Short answer: it depends a lot on **domain shift, data size/quality, and class balance**—but here are solid ballparks for a **frozen backbone + linear head** (“linear probe”):\n",
    "\n",
    "### What to expect (binary classification)\n",
    "\n",
    "* **Near-domain to ImageNet (natural photos, similar textures/objects):**\n",
    "  **85–95%+ accuracy** is common with a strong ImageNet-pretrained backbone and enough data (≥1–2k imgs/class).\n",
    "* **Moderate shift (e.g., satellite, document scans, some microscopy):**\n",
    "  **70–85%** is reasonable.\n",
    "* **Heavy shift (medical imaging like MR/CT/X-ray, histopathology), or very limited data (<200/class):**\n",
    "  **60–80%** is typical for a pure linear probe. If you’re below ~60%, something may be off; if you’re above ~90% quickly, double-check for leakage or shortcuts.\n",
    "\n",
    "### Fast sanity checks\n",
    "\n",
    "* **Baseline:** chance is **50%**. A decent linear probe should surpass **60–65%** quickly unless the task is intrinsically hard or labels are noisy.\n",
    "* **If class imbalance exists:** report **AUROC** (and **AUPRC**) in addition to accuracy; accuracy alone can be misleading.\n",
    "* **Learning curve:** plot accuracy/AUROC vs. train size. If it keeps rising, you’ve likely hit the limit of the linear head; consider unfreezing top blocks.\n",
    "\n",
    "### Practical tips for a reliable linear probe\n",
    "\n",
    "* **Freeze everything** in the backbone; train only a **single linear (or logistic) layer**.\n",
    "* **Use weight decay** (e.g., 1e-4 to 1e-3) and a small LR (1e-3 to 5e-3 for Adam).\n",
    "* **Early stop** on **val AUROC** (especially if imbalanced).\n",
    "* **Strong augmentations** help only if they don’t break label semantics; for medical images be conservative.\n",
    "* **Cross-validate** (e.g., 5-fold) to smooth variance on small datasets.\n",
    "\n",
    "### When to expect more\n",
    "\n",
    "* **Unfreezing the top 1–2 stages** (a “partial fine-tune”) often adds **+2–10 points** over a linear probe on shifted domains.\n",
    "* **Domain-specific pretraining** (e.g., medical self-supervised) can push a linear probe from the 60–80% band into the **75–90%** band on tough medical tasks.\n",
    "\n",
    "If you tell me roughly the domain (e.g., natural vs. medical), dataset size, and class balance, I can give a tighter expected range and a suggested training recipe.\n",
    "\n",
    "- Estimaited accuracy for fine-tuning linear prob is as follows: \n",
    "  - Accuracy: ~85–95% on a typical binary task (e.g., control vs. pathology), assuming decent sample size and clean labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Explanations about the model that I used**: \n",
    "\n",
    "- I use a pre-train model, this is either  self-supervised model or supervised model:\n",
    "- Last layer of pre-trained model in simclr is : `AdaptiveAvgPool2d(output_size=(1, 1))`\n",
    "- When I use a super-vised pre-trained model, the last layer again is: `(8): AdaptiveAvgPool2d(output_size=(1, 1))`\n",
    "- In fine-tune net, I compute logits without softmax function. So, the model returns logits, not probablities. \n",
    "- The embeddings vector has size 2048 in both simclr and supervised feature extractor (backbone: Resnet50). \n",
    "\n",
    "- **Option1**:\n",
    "  - Keep encoder frozen, only train the linear layer \n",
    "\n",
    "  - If we use this setting, something that we should backprobagate is logits not probablities \n",
    "\n",
    "  - **Methods that work well, with frozen backbone+ linear layer**:\n",
    "    - **CAM**: If the backbone ends up with `global average pooling` over spatial feature maps $A_k(x,y)$ and the head is linear with weights: `w_{c,k}`,\n",
    "    then CAM for class `c` is: $CAM_{c}(x,y)= \\sum_{k} w_{c,k}A_{k}(x,y)$\n",
    "\n",
    "      - No gradients needed; very stable.\n",
    "      - Perfect fit for linear probing on CNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the following, I used `EXP-002` for visualaising heatmaps\n",
    "- Then I tried `EXP--008` and `EXP-009` for visaulaising heatmaps with and without frozen encoder but on new split of data. \n",
    "- Note that when we use fine-tuned model with frozen or unfrozen encoder, indeed the perforamance seems simialr roughly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment registry\n",
    "\n",
    "| ID | Status | Backbone | Frozen | Head | Loss | LR | Epochs | Split | Metrics | Acc | dataset |\n",
    "|---:|:------:|:---------|:------:|:----|:-----|---:|------:|:------|:--------|:------|-------|\n",
    "| EXP-001 | ✅ done | ResNet50 | ✅ | Linear | CE | 1e-3 | 30 | subj 60/20/20 | Acc | tr:79, val:75 | uncorrupted\n",
    "| EXP-002 | ✅ done | ResNet50 | ✅ | Linear | CE | 1e-3 | 50 | subj 60/20/20 | Acc | tr:81 , val:76 | uncorrupted |\n",
    "| EXP-008 | ✅ done | ResNet50 | ✅ | Linear | CE | 1e-3 | 100 | subj 60/20/20 | Acc| tr:83 , val:76 | uncorrupted |\n",
    "| EXP-009 | ✅ done | unfreeze | ✅ | Linear | CE | 1e-3 | 100 | subj 60/20/20 | Acc | tr:83, val:75 | uncorrupted |\n",
    "| EXP-10  | Runing  | ResNet50 | ✅ | Linear | CE | 1e-3 | 50  | subj 60/20/20 | Acc | tr:81, val:76 | uncorrupted |\n",
    " EXP-003 | ✅ done | ResNet50 | ✅ | Linear | CE | 1e-3 | 50 | subj 60/20/20 | Acc | tr:68 , val:58 | corrupted(blur) |\n",
    "| EXP-004 | ✅ done | ResNet50 | ✅ | Linear | CE | 1e-3 | 30 | subj 60/20/20 | Acc |  | corrupted(zero)|\n",
    "| EXP-005 | ✅ done | ResNet50 | ✅ | Linear | CE | 1e-3 | 30 | subj 60/20/20 | Acc |tr:89,val:89| corrupted(zero32)|\n",
    "| EXP-006 | ✅ done | ResNet50 | ✅ | Linear | CE | 1e-3 | 60 | subj 60/20/20 | Acc |tr:92,val:91| corrupted(zero32)|\n",
    "| EXP-007 | ✅ done | ResNet50 | ✅ | Linear | CE | 1e-3 | 100 | subj 100/20/20 | Acc |tr:,val:| corrupted(zero32)|\n",
    "| EXP-14  | ✅ done | ResNet50 | ✅ |last-layer|CE| 1e-3 | 50  |                | Acc |tr:86,val:82|uncorrupted|\n",
    "|         |         |          |    |         |  |  1e-4|     |                 |    |            |           |\n",
    "| EXP-13  | ✅ done | ResNet50 | ✅  |         |  | same | 5   |                 |Acc |tr:1.,v1l:1 |corrupted(zero32)|  \n",
    "| EXP-16  | ✅ done | ResNet50 | ✅  |         |  | same | 10  |                 | Acc |tr:1,val:.99|corr(circ) |\n",
    "| EXP-11  | best   |          |    | whmodel |   | same |     |                 | Acc|            |uncorrupted |\n",
    "| EXP-12  | best   |          |    |lastblock|   | same |     |                 | Acc|            |incorrupted |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| ID | p-value | test-statistic | num-sampels | method | groups | \n",
    "|---:|:------:|:---------|:------:|:----|:-----|\n",
    "| EXP-008 | 0.0010 | 1.2552 | 100 | cam | uncor |\n",
    "| EXP-009 | 0.001 | 1.27 | 100 | cam | uncor | \n",
    "| EXP-10  | 0.001 | 1.25 | 100 | cam | uncor |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset part:\n",
    "\n",
    "- For creating dataset I split dataset into : train:val:test sets with percentages: [60,20,20]\n",
    "- I fine tun model on train:val and I use test-split for test-statistic and making heatmaps. \n",
    "- In uncorrupted images: we have two groups one with big Hippocampus, another one with small hippocampus\n",
    "- In corrrupted images: we have one group and the second group is the same as first group with artificats added to it. \n",
    "- Note that I use train and test parts for fine-tuning and only test part for making heatmaps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P-value and test-statistic when we corrupt images "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Corrupted | p-val     |test-statist|numsamples|\n",
    "|-----------|-----------|-----------|-----------|\n",
    "|blur(ps=32)|$9.9\\times 10^{-4}$| 0.235     | 4591|\n",
    "|zero-out   |$9.9\\times 10^{-4}$|  0.669    | 4591|\n",
    "|het_zero(4)|$0.001$   |5.9189 | 64  |\n",
    "|no_cor     |$9\\times 10^{-4}$| 1.68 | 100|\n",
    "|no_cor     |$9\\times 10^{-4}$| 0.54 |4591|\n",
    "|no_cor (test-set)|           |1.2267 | 100 | \n",
    "|cor_p32_zer|     | 0.4110| 100|\n",
    "|non_cor_2gr|0.0010|1.5048| 100|\n",
    "|cor_p32_zer(exp7)|0.0559 | 0.1514(0.38)| 100|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps that I want to take\n",
    "\n",
    "- Use, different explenation methods on validation set to see which one gives better results (supervised manner)\n",
    "- Use test-statistic and the same methods of previus methods to see if you can visualaise something meaningful. \n",
    "- After validating results on validation set go to test-set\n",
    "\n",
    "- Then I would check if back-probagating tets-statistic will also work or not \n",
    "\n",
    "- **Something that I want to do today**:\n",
    "  - Check zero-out setting and fine-tune model to see if you can get better accuracy on this setting \n",
    "\n",
    "- **Requirenments for obtaining heatmaps**:\n",
    "  - I should not have had \"with torch_no_grad\" in my pipeline when I want to make attributions.\n",
    "  - So, I should transfer model to \"evaluation\" mood, but not including \"with torch.no_grad\" to obtain heatmaps. \n",
    "\n",
    "- **ToDo list for today**:\n",
    "  - Check pre-trained and fine-tuned model on Brain-MRI heat maps for visualising heatmpas for these tow original groups that I have. => It was done for **supervsied fine-tuning for both GradCAM and GradCAM++**, **pre-training was done for visualisation**  \n",
    "  - Check another EX-AI method to see how heatmpas look like: => **It was done only for GradCAM++**\n",
    "  - If I have more time check Control group \n",
    "  \n",
    "- **Results**:\n",
    "  - Fine-tuning is quite importnat to get meaningful results\n",
    "  - Both GradCAM and GradCAM++ give me the same heatmaps \n",
    "  - Pre-trained model without fine-tuning does not give meaningful heatmpaps. \n",
    "\n",
    "\n",
    "- **ToDo list for today**:\n",
    "  - Check why GradCAM++ has the same results as GradCAM\n",
    "  - Moreover, check results on Testset => **It was done. Results on Test-set had the same heatmaps as on other datasets**\n",
    "  - Check two other explainaibilty methods. => **I used IG and noise tunnle using Captum Library, but I did not get good results** => I tried to use other two methods GradCA++ and LyaerCam  and they gave me the same resuts as GradCAM. \n",
    "\n",
    "    - I tried to use other metods like IG, etc but they did not work. \n",
    "\n",
    "- **ToDo list for today**:\n",
    "  - Check why GradCAM++ gives us the same results as GradCAM\n",
    "\n",
    "- **Chat with chatGPT about EXAI methods**:\n",
    "  - https://chatgpt.com/c/68e8fad7-1f38-8329-91fd-1851a7f84b79\n",
    "  \n",
    "  - **Other XAI methods**: \n",
    "    - **IG**: Integrates gradients along a path from baseline → input (Per-pixel quantitative attributions that satisfy completeness; useful for comparing groups statistically)\n",
    "\n",
    "    - **SmoothGrad**: Adds Gaussian noise to input and averages gradients (Reduces gradient noise → smoother and more stable saliency maps)\n",
    "\n",
    "    - **Guided Grad-CAM**: Restricts gradients to positive ReLU paths -> Produces sharper, more interpretable boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Next steps based on waht chatGPT said**:\n",
    "\n",
    "Awesome — thanks for the details. You’re in a great spot to get **quantitative, group-comparable explanations** beyond Grad-CAM/++. Below is a concrete, ResNet-friendly roadmap that plugs directly into your “mean-embedding L2 gap” setup.\n",
    "\n",
    "---\n",
    "\n",
    "# What to add (and how)\n",
    "\n",
    "## 1) “Directional” attribution in input space (IG / Grad×Input / DeepLIFT)\n",
    "\n",
    "You’re currently backpropagating the scalar\n",
    "( s=\\lVert \\mu_A-\\mu_B\\rVert_2 ) with (\\mu_g=\\frac{1}{N_g}\\sum_{x\\in g} f(x)) (where (f(x)) is the embedding).\n",
    "\n",
    "For a sample (x\\in A), the exact gradient of (s) w.r.t. its embedding is:\n",
    "[\n",
    "\\frac{\\partial s}{\\partial f(x)} ;=; \\frac{1}{N_A},\\frac{\\mu_A-\\mu_B}{\\lVert \\mu_A-\\mu_B\\rVert_2};=;\\frac{1}{N_A},w,\n",
    "\\quad\\text{with } w:=\\frac{\\mu_A-\\mu_B}{\\lVert \\mu_A-\\mu_B\\rVert_2}.\n",
    "]\n",
    "So, per sample, you can equivalently attribute **the directional projection**\n",
    "[\n",
    "\\phi_A(x) ;=; w^\\top f(x),/,N_A \\quad (\\text{and } \\phi_B(x)=-,w^\\top f(x),/,N_B \\text{ for } x\\in B)\n",
    "]\n",
    "to the **input pixels** using any attribution method (IG, Grad×Input, DeepLIFT). This gives **pixel-level** importance for “moving the embedding along the discriminative direction (w)”—a perfect match to your statistic.\n",
    "\n",
    "**How to implement (PyTorch/Captum idea):**\n",
    "\n",
    "* Freeze (w) (computed once from the full dataset).\n",
    "* Define a forward that returns (\\phi_A(x)) or (\\phi_B(x)).\n",
    "* Run **Integrated Gradients** on that forward to get faithful, additive attributions per pixel.\n",
    "* Baseline: use a black/mean image or a few realistic baselines and **average** IG (aka *random baselines* or *GradientSHAP*).\n",
    "\n",
    "This will give you **numerical, per-pixel attributions** you can aggregate across subjects and compare between groups (e.g., voxelwise t-test on IG maps).\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Causal validation via perturbation (Occlusion / RISE)\n",
    "\n",
    "To complement gradients:\n",
    "\n",
    "* **Occlusion (patch masking):** verify that masking *high-IG* regions causes the **projection score** (\\phi(x)) (or the original classifier logit) to drop more than masking random regions.\n",
    "* **RISE:** random masks → importance by correlation with (\\phi(x)). Gradient-free, model-agnostic sanity check.\n",
    "\n",
    "This strengthens causal claims (“these pixels matter for the A↔B separation”).\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Shapley-style for uncertainty & interactions (GradientSHAP / DeepSHAP)\n",
    "\n",
    "* **GradientSHAP** over (\\phi(x)): samples random baselines, approximates Shapley values, handles feature correlation better than vanilla IG.\n",
    "* **DeepSHAP** (if you can define a background set): additive, fast, and gives **distributions** of attribution per region you can compare between cohorts.\n",
    "\n",
    "Great when you want **confidence intervals** or **per-ROI importance distributions**.\n",
    "\n",
    "---\n",
    "\n",
    "## 4) Concept-level explanations (TCAV / ACE) on the embedding\n",
    "\n",
    "Your pipeline already lives in embedding space—perfect for **concept analysis**:\n",
    "\n",
    "* Define clinical concepts (e.g., “medial temporal horn widening”, “hippocampal head shape”, “perihippocampal CSF signal”) via small prototypical patches or curated examples.\n",
    "* Learn **Concept Activation Vectors** (CAVs) in the embedding or an internal layer.\n",
    "* Run **TCAV** to quantify how much each concept drives the A vs. B separation (on the classifier logit or on (\\phi(x))).\n",
    "\n",
    "This connects the statistical separation to **interpretable, clinician-friendly concepts**.\n",
    "\n",
    "---\n",
    "\n",
    "## 5) Region-of-interest (ROI) aggregation & group stats\n",
    "\n",
    "Once you’ve got per-pixel attributions (IG/SHAP):\n",
    "\n",
    "* **Normalize** per image (e.g., sum to total contribution or z-score).\n",
    "* **Aggregate per ROI** (hippocampus subfields, surrounding structures): mean/median attribution per ROI.\n",
    "* **Group-level tests:** permutation tests or mixed-effects models on ROI attributions; control multiple comparisons (FDR).\n",
    "* **Stability checks:** split-half reliability; correlate maps across seeds/augmentations; Adebayo-style **model/randomization tests** to ensure saliency isn’t label-independent.\n",
    "\n",
    "---\n",
    "\n",
    "# Minimal, drop-in recipe (Captum-style)\n",
    "\n",
    "**Step 0 — compute (w):**\n",
    "\n",
    "1. Run the model once over the dataset to collect embeddings.\n",
    "2. ( \\mu_A, \\mu_B \\leftarrow ) per-group means; ( w \\leftarrow (\\mu_A-\\mu_B)/\\lVert\\mu_A-\\mu_B\\rVert ).\n",
    "\n",
    "**Step 1 — define a forward for attribution:**\n",
    "\n",
    "```python\n",
    "# Pseudocode\n",
    "def forward_phi(x, model, w, group, N_group):\n",
    "    emb = model.backbone_to_embedding(x)   # the layer you used for µ\n",
    "    # emb shape: [B, D]; w shape: [D]\n",
    "    proj = (emb @ w) / N_group              # φ_A or φ_B depending on group of x\n",
    "    # Return a scalar per sample for Captum (sum over batch if needed)\n",
    "    return proj\n",
    "```\n",
    "\n",
    "**Step 2 — run IG (or GradientSHAP):**\n",
    "\n",
    "* Baselines: a small set of realistic images (e.g., mean intensity, mild Gaussian-blurred versions).\n",
    "* Call `IntegratedGradients(forward_phi).attribute(inputs=x, baselines=baselines, n_steps=50–200)`.\n",
    "* Optionally also compute **Grad×Input** for speed and **SmoothGrad** for denoising.\n",
    "\n",
    "**Step 3 — validate causality:**\n",
    "\n",
    "* Occlusion sensitivity on top-k% IG pixels vs. random pixels, measure drop in (\\phi(x)) or the classifier logit.\n",
    "\n",
    "**Step 4 — aggregate & test:**\n",
    "\n",
    "* Warp IG maps to a common space (if needed), compute ROI means, run group t-tests or permutation tests; report FDR-corrected p-values.\n",
    "\n",
    "---\n",
    "\n",
    "# Extra ideas that work well with ResNet\n",
    "\n",
    "* **Guided Grad-CAM on the discriminative head:** Train a **linear probe** on embeddings to predict A vs. B (if you aren’t already). Then run Grad-CAM++ on the **probe logit** (often sharper than on the original binary head if it differs).\n",
    "* **Layerwise Relevance Propagation (LRP-ε/γ):** Good for CNNs; gives input-space, conservation-style attributions. Useful as a third family to triangulate with IG and Occlusion.\n",
    "* **Counterfactual editing:** Optimize a tiny, smooth perturbation (\\delta) that maximally **reduces** (w^\\top f(x)) s.t. (\\lVert\\delta\\rVert) small, then visualize (\\delta). This yields intuitive “what to change” maps (keep within safety; no data leakage).\n",
    "\n",
    "---\n",
    "\n",
    "# What this gives you beyond Grad-CAM/++\n",
    "\n",
    "* **Pixel-level, additive attributions** aligned with your **exact discriminative statistic** ((\\phi)).\n",
    "* **Causal validation** that masking high-importance pixels changes the statistic or the decision.\n",
    "* **Concept-level interpretability** that clinicians can understand and you can test statistically.\n",
    "* **Group-level hypothesis testing** on attribution maps/ROIs with proper error control.\n",
    "\n",
    "If you want, I can sketch precise Captum code for your ResNet (which layer to hook for embeddings, how to package (w) and choose baselines, and a small Occlusion validator) — just say PyTorch or TF/Keras and the shape of your inputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ways that we can see if the method visualsiez biomarkers that are relavnt to biomarkers:\n",
    "\n",
    "- Great—here’s a tight, *do-it-now* recipe to quantitatively link your explanations to the volumetric biomarker.\n",
    "\n",
    "# Goal\n",
    "\n",
    "Test whether the model’s attribution maps (Grad-CAM/IG/SHAP) **track hippocampal volume** (a known biomarker), and whether they add **finer subregional signal** beyond volume.\n",
    "\n",
    "---\n",
    "\n",
    "# Data you need per subject\n",
    "\n",
    "* `A_i`: attribution map in image space (same voxel grid as your MRI).\n",
    "* `H_i^L`, `H_i^R`: binary masks for left/right hippocampus (or subfield masks if you have them).\n",
    "* `Vol_i^L`, `Vol_i^R`: hippocampal volumes (mm³ or voxels × voxel_volume).\n",
    "* Covariates: `Age_i`, `Sex_i`, `TIV_i` (total intracranial volume), `Scanner_i`, etc.\n",
    "\n",
    "---\n",
    "\n",
    "# 1) Define robust attribution summaries (per hemisphere)\n",
    "\n",
    "Because absolute scale varies across images, compute **normalized** ROI-wise attribution:\n",
    "\n",
    "* Choose signed or absolute attributions:\n",
    "\n",
    "  * **Signed** (good for directional IG/SHAP aligned with your ( w ) projection).\n",
    "  * **Absolute** (good for Grad-CAM-type magnitude maps).\n",
    "\n",
    "Per subject & hemisphere:\n",
    "[\n",
    "S_i^h ;=; \\frac{\\sum_{v\\in H_i^h} A_i(v)}{\\sum_{v \\in \\Omega_i} |A_i(v)| + \\epsilon}\n",
    "\\quad\\text{or}\\quad\n",
    "\\tilde S_i^h ;=; \\frac{1}{|H_i^h|}\\sum_{v\\in H_i^h} A_i(v)\n",
    "]\n",
    "Use both a **relative** score (S) and a **mean-ROI** score (\\tilde S) for sensitivity analysis.\n",
    "\n",
    "**Negative control:** compute the same in a control ROI (e.g., occipital cortex) to ensure specificity.\n",
    "\n",
    "---\n",
    "\n",
    "# 2) Basic association tests\n",
    "\n",
    "For each hemisphere (and combined):\n",
    "\n",
    "* **Correlation:** Pearson (linear) + Spearman (rank):\n",
    "  [\n",
    "  \\text{corr}(S_i^h, ; Vol_i^h)\n",
    "  ]\n",
    "* **Partial correlation** controlling for covariates (Age, Sex, TIV, Scanner).\n",
    "* **Permutation test** (e.g., 10k label shuffles) for robust p-values.\n",
    "\n",
    "**Interpretation:** Significant positive correlation → higher explanatory mass where volume is larger (or negative if your signed attribution decreases with larger volume). Do this for **Grad-CAM**, **IG**, and **GradientSHAP** to compare families.\n",
    "\n",
    "---\n",
    "\n",
    "# 3) Regression with covariate control\n",
    "\n",
    "Fit a small model:\n",
    "[\n",
    "Vol_i^h ;=; \\beta_0 + \\beta_1 S_i^h + \\beta_2 \\text{Age}_i + \\beta_3 \\text{Sex}_i + \\beta_4 \\text{TIV}_i + \\beta_5 \\text{Scanner}_i + \\varepsilon_i\n",
    "]\n",
    "\n",
    "* Report (\\beta_1) (SE, p-value), partial (R^2) for (S).\n",
    "* If repeated scans/centers → mixed-effects with random intercepts per subject/center.\n",
    "\n",
    "**Ablation:** compare base model (covariates only) vs. base + (S). ΔAIC/Δ(R^2) shows added value of the attribution.\n",
    "\n",
    "---\n",
    "\n",
    "# 4) Subfield analysis (if available)\n",
    "\n",
    "Repeat #2–3 for CA1, CA2/3, CA4/DG, Subiculum masks:\n",
    "\n",
    "* See which subfields carry the strongest attribution–volume link.\n",
    "* FDR-correct across subfields.\n",
    "\n",
    "---\n",
    "\n",
    "# 5) Causal validation by occlusion\n",
    "\n",
    "* For each image, zero out the **top-k% attribution voxels within H** and recompute:\n",
    "\n",
    "  * your **projection score** (\\phi(x) = w^\\top f(x)),\n",
    "  * and/or the **classification logit**.\n",
    "* Compare the drop against masking random hippocampal voxels or the same k% in the control ROI.\n",
    "  A larger drop for top-k confirms causal relevance.\n",
    "\n",
    "---\n",
    "\n",
    "# 6) Mediation check (optional but nice)\n",
    "\n",
    "If your label is Large vs Small:\n",
    "\n",
    "* Regress Label ~ Volume (+ covariates).\n",
    "* Add (S) to the model. If Volume’s coefficient shrinks and (S) is significant, attributions may reflect **how** the model encodes volumetry (and possibly more).\n",
    "\n",
    "---\n",
    "\n",
    "# 7) Compact Python (NumPy/Pandas + nibabel/nilearn)\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def roi_summary(A, mask, whole_abs_norm=True, eps=1e-8):\n",
    "    roi_vals = A[mask > 0]\n",
    "    if whole_abs_norm:\n",
    "        denom = np.abs(A).sum() + eps\n",
    "        return roi_vals.sum() / denom\n",
    "    else:\n",
    "        return roi_vals.mean()\n",
    "\n",
    "def partial_corr(y, x, covars):\n",
    "    # Regress out covariates and correlate residuals\n",
    "    Xc = np.column_stack([np.ones(len(covars)), covars])\n",
    "    def resid(z):\n",
    "        beta, *_ = np.linalg.lstsq(Xc, z, rcond=None)\n",
    "        return z - Xc @ beta\n",
    "    return pearsonr(resid(y), resid(x))\n",
    "\n",
    "# Example per-hemisphere loop\n",
    "# df must contain columns: subj_id, S_L, S_R, Vol_L, Vol_R, Age, Sex, TIV, Scanner\n",
    "def analyze(df, side='L'):\n",
    "    y = df[f'Vol_{side}'].values\n",
    "    x = df[f'S_{side}'].values\n",
    "    covars = pd.get_dummies(df[['Age','Sex','TIV','Scanner']], drop_first=True).values\n",
    "\n",
    "    r_p = pearsonr(x, y)\n",
    "    r_s = spearmanr(x, y)\n",
    "    r_pc, p_pc = partial_corr(y, x, covars)\n",
    "\n",
    "    # Linear regression with covariates\n",
    "    X = np.column_stack([x, covars])\n",
    "    lr = LinearRegression().fit(X, y)\n",
    "    beta_S = lr.coef_[0]\n",
    "    r2 = lr.score(X, y)\n",
    "\n",
    "    return dict(pearson=r_p, spearman=r_s, partial=(r_pc, p_pc),\n",
    "                beta_S=beta_S, r2=r2)\n",
    "\n",
    "# Permutation test (partial correlation)\n",
    "def perm_test_partial(y, x, covars, n_perm=10000, rng=0):\n",
    "    rng = np.random.default_rng(rng)\n",
    "    r_pc_obs, _ = partial_corr(y, x, covars)\n",
    "    cnt = 0\n",
    "    for _ in range(n_perm):\n",
    "        x_perm = rng.permutation(x)\n",
    "        r_pc_perm, _ = partial_corr(y, x_perm, covars)\n",
    "        cnt += (abs(r_pc_perm) >= abs(r_pc_obs))\n",
    "    p = (cnt + 1) / (n_perm + 1)\n",
    "    return r_pc_obs, p)\n",
    "```\n",
    "\n",
    "> Swap in your own I/O to load `A_i` and masks, compute `S_L/S_R`, then run `analyze` and `perm_test_partial`.\n",
    "\n",
    "---\n",
    "\n",
    "# 8) Reporting checklist\n",
    "\n",
    "* Correlations (Pearson/Spearman) + partial correlations with 95% CIs.\n",
    "* Regression tables with ∆(R^2) vs. covariate-only baseline.\n",
    "* Subfield results with FDR control.\n",
    "* Negative-control ROI (specificity).\n",
    "* Occlusion drop curves (top-k vs random; show mean ± CI).\n",
    "\n",
    "---\n",
    "\n",
    "# Quick pitfalls (avoid these)\n",
    "\n",
    "* **Miscaligned spaces:** ensure attribution maps and masks are in the **same space** (post-affine & resampling).\n",
    "* **Leakage via normalization:** normalize per-image, not across the cohort.\n",
    "* **Multiple comparisons:** correct if you scan many ks, ROIs, or methods.\n",
    "* **Scanner/site effects:** include site fixed effects or random intercepts.\n",
    "\n",
    "---\n",
    "\n",
    "If you’d like, tell me:\n",
    "\n",
    "* how you store your attributions (NIfTI? NumPy?),\n",
    "* what masks you have (whole hippocampus vs. subfields),\n",
    "* and which attribution (Grad-CAM, IG, GradientSHAP) you want to start with,\n",
    "\n",
    "and I’ll tailor the exact loading/processing code and the occlusion validator for your setup.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If I split corrupted images:\n",
    "\n",
    "  - Using corrupted images with zeroing with ps=32 for group 1\n",
    "  - idx:[ 174 1444 2235 ...  491 2189 1672] in split_60_20_20_per_class\n",
    "  - idx:[3930  652 1657 ... 4392 1346 1709] in split_60_20_20_per_class\n",
    "\n",
    "- Now I call function for uncropted images:\n",
    "\n",
    "  - Using uncorrupted images it also gives the following indiecs for each group:\n",
    "   - idx:[ 174 1444 2235 ...  491 2189 1672] in split_60_20_20_per_class\n",
    "   - idx:[3930  652 1657 ... 4392 1346 1709] in split_60_20_20_per_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I extracted these parts from my code:\n",
    "\n",
    "  - They are related to my prevoius code that I corrupted some regions in the image:\n",
    "    - if self.args.deg == '4': \n",
    "                ADNI_CORR = Path(\"/sc/home/masoumeh.javanbakhat/netstore-old/Baysian/3D/Explainability/Retina_Codes/adni_corrupted\")\n",
    "                path_corrupted = os.path.join(ADNI_CORR, 'group1_masked_64_4.npy')\n",
    "                corrupted = np.load(path_corrupted)\n",
    "                self.group1_np = corrupted[:self.args.m]\n",
    "\n",
    "    - if self.args.deg == '5': \n",
    "                ADNI_CORR = Path(\"/sc/home/masoumeh.javanbakhat/netstore-old/Baysian/3D/Explainability/Retina_Codes/adni_corrupted\")\n",
    "                path_corrupted = os.path.join(ADNI_CORR, 'group1_masked_64_5.npy')\n",
    "                corrupted = np.load(path_corrupted)\n",
    "                self.group1_np = corrupted[:self.args.m]\n",
    "                print(f'5% of most important pixels were corrupted for group 1')\n",
    "\n",
    "    - if self.args.deg == '6': \n",
    "                ADNI_CORR = Path(\"/sc/home/masoumeh.javanbakhat/netstore-old/Baysian/3D/Explainability/Retina_Codes/adni_corrupted\")\n",
    "                path_corrupted = os.path.join(ADNI_CORR, 'group1_masked_64_6.npy')\n",
    "                corrupted = np.load(path_corrupted)\n",
    "                self.group1_np = corrupted[:self.args.m]\n",
    "                print(f'6% of most important pixels were corrupted for group 1')\n",
    "\n",
    "    - if self.args.deg == 'bl6': \n",
    "                ADNI_CORR = Path(\"/sc/home/masoumeh.javanbakhat/netstore-old/Baysian/3D/Explainability/Retina_Codes/adni_blured\")\n",
    "                path_corrupted = os.path.join(ADNI_CORR, 'group1_blured_64_5.npy')\n",
    "                corrupted = np.load(path_corrupted)\n",
    "                self.group1_np = corrupted[:self.args.m]\n",
    "                print(f'6% of most important pixels were blured for group 1')\n",
    "\n",
    "    - "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
